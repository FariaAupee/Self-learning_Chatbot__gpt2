{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "! pip -q install transformers"
      ],
      "metadata": {
        "id": "3jYidZXiGPMD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIhcQRynjTT1",
        "outputId": "868a151a-1503-4c25-b237-3b9f5cbe96a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "cQlzcQKYFZ2P"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "\n",
        "class ChatData(Dataset):\n",
        "    def __init__(self, path:str, tokenizer):\n",
        "        self.data = json.load(open(path, \"r\",  encoding=\"utf-8\"))\n",
        "\n",
        "        self.X = []\n",
        "        for i in self.data:\n",
        "            for j in i['dialog']:\n",
        "                self.X.append(j['text'])\n",
        "\n",
        "        for idx, i in enumerate(self.X):\n",
        "            try:\n",
        "                self.X[idx] = \"<startofstring> \" +i+\" <bot>: \"+self.X[idx+1]+\" <endofstring>\"\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        self.X = self.X[:5000]\n",
        "        \n",
        "        print(self.X[0])\n",
        "\n",
        "        self.X_encoded = tokenizer(self.X, max_length=42, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.input_ids[idx], self.attention_mask[idx])"
      ],
      "metadata": {
        "id": "AY016DuhFqQI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(inp):\n",
        "    inp = \"<startofstring> \"+inp+\" \"\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "    output = model.generate(X, attention_mask=a, max_length=60, temperature=0.5, top_p=0.9)\n",
        "    # print(output)\n",
        "    output = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "    return output"
      ],
      "metadata": {
        "id": "E2nalP6bFgmg"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "HuksWdBrkMcq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "chatData = ChatData(\"/content/drive/MyDrive/Chatbot/newdata.json\", tokenizer)\n",
        "chatData =  DataLoader(chatData, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8futmGkzcID",
        "outputId": "a73c9ea4-9fc3-4917-ca56-09b5c30a3447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<startofstring> Hi <bot>: Hello! How can I assist you today? <endofstring>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training loop\n",
        "\n",
        "model.train()\n",
        "\n",
        "optim = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# print(\"training .... \")\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for i in tqdm.tqdm(range(epochs)):\n",
        "    lss = 0\n",
        "    for X, a in chatData:\n",
        "        X = X.to(device)\n",
        "        a = a.to(device)\n",
        "        optim.zero_grad()\n",
        "        loss = model(X, attention_mask=a, labels=X).loss\n",
        "        lss = loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    print(\"loss: \", lss)\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/Chatbot/model_state.pt\")\n",
        "    print(infer(\"hello how are you\"))\n"
      ],
      "metadata": {
        "id": "CIITm6cIFjGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the saved model\n",
        "state_dict = torch.load('/content/drive/MyDrive/Chatbot/model_state.pt')\n",
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KuqCQOiuKGq",
        "outputId": "14922c24-ff42-4963-b78f-12728b69d90b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chatbot interaction and self-learning\n",
        "def collect_feedback(question, model, tokenizer, knowledge):\n",
        "    answer = infer(question)#, model, tokenizer, knowledge)\n",
        "    feedback = input(f\"Was the response helpful? (yes/no/skip): \")\n",
        "    if feedback.lower() == \"yes\":\n",
        "        return None\n",
        "    elif feedback.lower() == \"no\":\n",
        "        correct_answer = input(\"Bot: Please provide the correct answer: \")\n",
        "        return correct_answer\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Load or create the knowledge dictionary\n",
        "knowledge_path = \"/content/drive/MyDrive/Chatbot/knowledge.json\"\n",
        "try:\n",
        "    with open(knowledge_path, \"r\") as f:\n",
        "        knowledge = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "while True:\n",
        "    print(\"\\n\")\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input == 'exit':\n",
        "        break\n",
        "\n",
        "    if user_input.lower() in knowledge:\n",
        "        response = knowledge[user_input.lower()]\n",
        "        print(\"Bot:\", response)\n",
        "        feedback = collect_feedback(user_input, model, tokenizer, knowledge)\n",
        "        if feedback is not None:\n",
        "            knowledge[user_input.lower()] = feedback\n",
        "            print(\"Bot: Thanks. I've learned something new\")\n",
        "    else:\n",
        "        response = infer(user_input)\n",
        "        response = response.replace(user_input,\"\")\n",
        "        response = response.replace('<bot>:',\"\")\n",
        "        print(\"Bot:\", response)\n",
        "        feedback = collect_feedback(user_input, model, tokenizer, knowledge)\n",
        "        if feedback is not None:\n",
        "            knowledge[user_input.lower()] = feedback\n",
        "            print(\"Bot: Thanks. I've learned something new\")\n",
        "# Save the knowledge dictionary\n",
        "with open(knowledge_path, \"w\") as f:\n",
        "    json.dump(knowledge, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-qtW6lZ-39s",
        "outputId": "f81c72b5-a4a3-4be4-fb49-503ac459b26e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "User: hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: hello ಠ_ಠ: This is a widely used greeting in Bangladesh. It is often used to greet someone in a formal or informal setting. <bot>: Thank you for sharing! It's helpful to know the different ways we can say goodbye in different languages. <bot>:\n",
            "Was the response helpful? (yes/no/skip): no\n",
            "Bot: Please provide the correct answer: Hello. How can I help you?\n",
            "Bot: Thanks. I've learned something new\n",
            "\n",
            "\n",
            "User: hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Hello. How can I help you?\n",
            "Was the response helpful? (yes/no/skip): skip\n",
            "\n",
            "\n",
            "User: Hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: hi, how may I help you?\n",
            "Was the response helpful? (yes/no/skip): yes\n",
            "\n",
            "\n",
            "User: What do you know about Bangladesh?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:    Bangladesh is known for its vibrant and picturesque villages. The rural areas of the country are characterized by lush green fields, rivers, and a close-knit community. The villages offer a glimpse into the lives of the people, showcasing the rich\n",
            "Was the response helpful? (yes/no/skip): skip\n",
            "\n",
            "\n",
            "User: street food of bangladesh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  vernacular cuisine is thriving in Bangladesh. Some popular street food include biryani, fish curry, shorshe ilish (hilsa fish in mustard sauce), pitha (rice cakes), and bhapa pitha (spicy p\n",
            "Was the response helpful? (yes/no/skip): skip\n",
            "\n",
            "\n",
            "User: Can you recommend any famous street food in Bangladesh?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  Tell me about it.  Sure! In Bangladesh, you must try fuchka (pani puri), jhalmuri (spicy puffed rice), shingara (samosa), and bhapa pitha\n",
            "Was the response helpful? (yes/no/skip): skip\n",
            "\n",
            "\n",
            "User: Tell me about the Sundarbans\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:   in Bangladesh.  The Sundarbans is a UNESCO World Heritage Site and the largest mangrove forest in the world. It is home to the Royal Bengal Tigers and various unique flora and fauna. \n",
            "Was the response helpful? (yes/no/skip): skip\n",
            "\n",
            "\n",
            "User: Thank you. Goodbye for now\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: ! What are some common farewell greetings?  You're welcome! Farewell greetings provide a polite and respectful way to end a conversation or part ways. \n",
            "Was the response helpful? (yes/no/skip): skip\n",
            "\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    }
  ]
}